---
title: 'RL入门'
date: '2025-07-11'
tags: ['强化学习']
---

# Lecture 1

##  详细解读

### 1 强化学习概览

*   **RL的核心要素 (Slides 15-19)**:
    *   **优化 (Optimization)**: 目标是找到最优的决策方式，产生最佳或非常好的结果，涉及到明确的决策效用概念（例如，找到两城市间最短路径）。
    *   **延迟后果 (Delayed Consequences)**: 当前的决策会影响未来很长时间的结果。这引入了两个挑战：
        *   **规划时**: 决策不仅要考虑即时利益，还要考虑长期影响。
        *   **学习时**: 时间信用分配困难（即，什么导致了后来的高或低奖励？）。
    *   **探索 (Exploration)**: 通过做决策来了解世界。智能体扮演科学家的角色，通过尝试（和失败）来学习（例如，学习骑自行车）。决策会影响我们能学到什么（只获得已做决策的奖励，不知道未做决策的结果）。
    *   **泛化 (Generalization)**: 策略是将过去的经验映射到行动。RL的目标是学习一个策略，而不是预先编程一个特定行为的策略。

### 2 序贯决策（不确定性下）导论

*   **智能体与世界的交互 (Slide 33)**:
    *   这是一个循环过程：智能体（Agent）采取**行动（Action）**，世界（World）响应并提供**观察（Observation）**和**奖励（Reward）**。
    *   **目标**: 选择行动以最大化未来总预期奖励，这通常需要在即时奖励和长期奖励之间进行权衡。
*   **序贯决策过程 (离散时间) (Slide 37)**:
    *   在每个时间步 $t$：
        *   智能体采取行动 $a_t$。
        *   世界根据 $a_t$ 更新，并发出观察 $o_t$ 和奖励 $r_t$。
        *   智能体接收 $o_t$ 和 $r_t$。
*   **历史 (History) (Slide 38)**:
    *   历史 $h_t$ 是过去观察、行动和奖励的序列：$(a_1, o_1, r_1, ..., a_t, o_t, r_t)$。
    *   智能体根据历史选择行动。
    *   **状态 (State)**: 被认为是决定接下来会发生什么的信息。状态是历史的函数：$s_t = f(h_t)$。
*   **马尔可夫假设 (Markov Assumption) (Slides 39-40)**:
    *   **信息状态**: 历史的充分统计量。
    *   状态 $s_t$ 满足马尔可夫性当且仅当：给定当前状态和行动 $a_t$，未来的状态 $s_{t+1}$ 与过去的完整历史 $h_t$ 无关。即
        $$p(s_{t+1} | s_t, a_t) = p(s_{t+1} | h_t, a_t)$$
    *   **为什么流行**: 简单，且可以通过将部分历史包含在状态中来满足。
    *   **影响**: 对计算复杂度、所需数据和最终性能有重大影响。？

### 3 马尔可夫决策过程 (MDP Model)

*   **MDP模型 (Slide 43)**: 智能体对世界如何变化的表示。
    *   **转移/动态模型**:
        $$p(s_{t+1} = s' | s_t = s, a_t = a)$$
        预测下一个智能体状态。
    *   **奖励模型**:
        $$r(s_t = s, a_t = a) = E[r_t | s_t = s, a_t = a]$$
        预测即时奖励。
*   **策略 (Policy) $\pi$ (Slides 45-46)**:
    *   策略 $\pi$ 决定了智能体如何选择行动。
    *   $\pi: S \rightarrow A$，从状态到行动的映射。
    *   **确定性策略**: $\pi(s) = a$ (在状态 $s$ 下总是采取行动 $a$)。
    *   **随机性策略**: $\pi(a|s) = Pr(a_t = a | s_t = s)$ (在状态 $s$ 下采取行动 $a$ 的概率)。

### 4 马尔可夫奖励过程 (MRP)

*   **定义 (Slide 54)**: MRP是一个马尔可夫链加上奖励。
    *   $S$ 是（有限）状态集合。
    *   $P$ 是描述状态转移的动态/转移模型。
    *   $R$ 是奖励函数 $R(s) = E[r_t | S_t = s]$ (从状态 $s$ 预期获得的即时奖励)。
    *   $\gamma \in [0, 1]$ 是折扣因子。
    *   **注意**: MRP中没有行动，奖励只取决于当前状态。

### 5 回报与价值函数 (Return & Value Function)

*   **视野 (Horizon) $H$ (Slide 56)**: 每次试验中的时间步数，可以是有限或无限的。
*   **回报 (Return) $G_t$ (Slide 56)**: 从时间步 $t$ 到视野 $H$ 的奖励的折扣和。
    $$G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots + \gamma^{H-1} r_{t+H-1}$$
*   **状态价值函数 (State Value Function) $V(s)$ (Slide 56)**: 对于马尔可夫奖励过程，是从状态 $s$ 开始的预期回报。
    $$V(s) = E[G_t | S_t = s] = E[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots + \gamma^{H-1} r_{t+H-1} | S_t = s]$$
*   **折扣因子 (Discount Factor) $\gamma$ (Slides 57-58)**:
    *   数学上方便（避免无限回报和价值）。
    *   人类通常表现出存在一个小于 $1$ 的折扣因子。
    *   $\gamma = 0$: 只关心即时奖励。
    *   $\gamma = 1$: 未来奖励与即时奖励同样有益（如果试验长度 $H$ 总是有限的话）。
*   **计算马尔可夫奖励过程的价值 (Computing the Value of a Markov Reward Process) (Slides 59-62)**:
    *   马尔可夫特性提供了结构。MRP价值函数满足**贝尔曼方程**：
        $$V(s) = R(s) + \gamma \sum_{s' \in S} P(s'|s)V(s')$$
        （即时奖励 + 未来折扣奖励之和）
    *   **矩阵形式的贝尔曼方程**: 对于有限状态MRP，可以表示为矩阵方程：
        $$V = R + \gamma PV$$
    *   **MRP价值的解析解 (Analytic Solution)**:
        $$V = (I - \gamma P)^{-1}R$$
        直接求解需要矩阵求逆，计算复杂度约为 $O(N^3)$，其中 $N$ 是状态数。
    *   **计算MRP价值的迭代算法 (Iterative Algorithm)**: 动态规划。
        1.  初始化 $V_0(s) = 0$ 对于所有 $s$。
        2.  对于 $k = 1$ 直到收敛：
            对于所有 $s$ in $S$：
            $$V_k(s) = R(s) + \gamma \sum_{s' \in S} P(s'|s)V_{k-1}(s')$$
        *   每次迭代的计算复杂度为 $O(|S|^2)$。

### 6 RL算法组件 (RL Algorithm Components)

*   RL算法通常包含以下一个或多个组件 (Slide 67)：**模型（Model）**、**策略（Policy）**、**价值函数（Value Function）**。
*   **价值函数 (Slide 68)**: $V^{\pi}$ 是在遵循给定策略 $\pi$ 的情况下，从状态 $s$ 开始的未来折扣奖励的预期和。
    $$V^{\pi}(S_t = s) = E_{\pi}[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + \dots |S_t = s]$$
    它可以用来量化状态和行动的"好坏"，并通过比较策略来决定如何行动。
*   **火星漫游车价值函数示例 (Discount Factor $\gamma = 0$) (Slide 69)**:
    如果折扣因子 $\gamma = 0$，那么价值函数 $V^{\pi}(S_t = s)$ 只等于即时奖励 $r(s)$，因为它不考虑未来的奖励。

### 7 RL智能体的类型 (Types of RL Agents)

RL智能体主要分为两类 (Slides 70-71)：

1.  **基于模型 (Model-based)**:
    *   明确地建立环境模型（例如，世界如何变化，行动的奖励）。
    *   可能包含或不包含策略和/或价值函数。
2.  **无模型 (Model-free)**:
    *   不明确建立环境模型。
    *   **基于价值 (Value-based)**: 明确地学习价值函数。
    *   **基于策略 (Policy-based)**: 明确地学习策略函数。
    *   **Actor-Critic (行动者-评论者)**: 结合了基于价值和基于策略的方法，通常Actor学习策略，Critic学习价值函数。

# Lecture 2

## 1. 关键要点 (Key Points)

-   **MDP定义**: 马尔可夫奖励过程（MRP）加上行动。
-   **MDP策略评估**: 通过将策略 $\pi$ 应用到MDP上，可以将其转化为MRP，然后使用MRP的价值评估技术来计算策略的价值。
-   **策略迭代 (Policy Iteration, PI)**:
    1.  **策略评估**: 计算当前策略的价值函数 $V^{\pi}(s)$。
    2.  **策略改进**: 根据当前的价值函数，贪婪地选择行动，从而得到一个新的、更好的策略 $\pi'$。
    *   策略迭代保证单调改进，并最终收敛到最优策略。
-   **价值迭代 (Value Iteration, VI)**:
    *   直接迭代贝尔曼最优方程来计算最优价值函数 $V^*(s)$。
    *   $V_{k+1}(s) = \max_a [R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a)V_k(s')]$。
    *   价值迭代的收敛性基于贝尔曼备份操作符（Bellman Backup Operator）是收缩映射的性质。
-   **Bellman 操作符**:
    *   **Bellman Backup Operator ($B^{\pi}$)**: 用于策略评估，将一个价值函数转换为另一个，并最终收敛到当前策略的真实价值。
    *   **Optimal Bellman Operator ($B^*$)**: 用于价值迭代，找到最优价值函数。
-   **收缩映射 (Contraction Operator)**: 确保价值迭代等算法收敛到唯一解的关键数学性质。当折扣因子 $\gamma < 1$ 时，贝尔曼备份操作符是收缩映射。
-   **最优策略的性质**: 在无限视野MDP中，最优策略是确定性且平稳（不随时间变化）的，但可能不唯一。在有限视野MDP中，最优策略通常不平稳（取决于剩余时间步）。
-   **策略数量**: 对于 $|S|$ 个状态和 $|A|$ 个行动，有 $|A|^{|S|}$ 个确定性策略。

---

## 2. 详细解读 (Detailed Breakdown)

### 2.1 马尔可夫决策过程（MDP）(Markov Decision Process (MDP))

*   **MDP定义**: MDP是一个**马尔可夫奖励过程 + 行动**。它是一个五元组：$(S, A, P, R, \gamma)$。
    *   $S$: 有限的马尔可夫状态集合 $s \in S$。
    *   $A$: 有限的行动集合 $a \in A$。
    *   $P$: 动态/转移模型，对于每个行动，它指定 $P(S_{t+1} = s'|S_t = s, a_t = a)$。
    *   $R$: 奖励函数 $R(S_t = s, a_t = a) = E[r_t|S_t = s, a_t = a]$。
    *   $\gamma$: 折扣因子 $\gamma \in [0, 1]$。
*   **MDP策略 ($\pi$)**: 定义在每个状态下采取什么行动（可以是确定性或随机性）。
    *   $\pi(a|s) = P(a_t = a|S_t = s)$。
*   **MDP + 策略 = MRP**: 将一个策略 $\pi$ 应用到MDP上，可以得到一个对应的MRP $(S, R^{\pi}, P^{\pi}, \gamma)$，其中：
    *   策略下的期望奖励 $R^{\pi}(s) = \sum_{a \in A} \pi(a|s)R(s, a)$。
    *   策略下的期望转移 $P^{\pi}(s'|s) = \sum_{a \in A} \pi(a|s)P(s'|s, a)$。
    这表明我们可以使用与MRP相同的方法来评估MDP中特定策略的价值。

### 2.2 MDP策略评估 (MDP Policy Evaluation)

*   **迭代算法**: 初始化 $V_0(s) = 0$。对于 $k=1$ 直到收敛，对所有 $s \in S$：
    $$V_k^{\pi}(s) = \sum_a \pi(a|s) \left[ R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a)V_{k-1}^{\pi}(s') \right]$$
    这被称为针对特定策略的**贝尔曼备份 (Bellman backup)**。如果策略是确定性的 $\pi(s)$，则简化为：
    $$V_k^{\pi}(s) = R(s, \pi(s)) + \gamma \sum_{s' \in S} P(s'|s, \pi(s)) V_{k-1}^{\pi}(s')$$

### 2.3 MDP控制：寻找最优策略 (MDP Control: Finding Optimal Policy)

*   **目标**: 计算最优策略 $\pi^*(s) = \arg \max_\pi V^{\pi}(s)$。
*   **最优价值函数**: 存在一个**唯一的**最优价值函数 $V^*(s)$。
*   **最优策略**: 对于无限视野问题，最优策略是确定性且平稳（不依赖时间步）的，但**不一定唯一**（可能有多个策略达到相同的最优价值）。
*   **策略数量**: 对于 $|S|$ 个状态和 $|A|$ 个行动，存在 $|A|^{|S|}$ 个确定性策略。因此，枚举所有策略并评估是不可行的。

### 2.4 策略迭代（PI）(Policy Iteration (PI))

策略迭代是一种通过交替进行策略评估和策略改进来寻找最优策略的算法：

1.  **初始化**: 随机设置初始策略 $\pi_0(s)$。
2.  **循环**: 当策略不再改变（$||\pi_i - \pi_{i-1}||_1 = 0$）时停止。在每次迭代 $i$ 中：
    *   **策略评估 (Policy Evaluation)**: 计算当前策略 $\pi_i$ 的价值函数 $V^{\pi_i}$。这可以通过上述迭代算法（贝尔曼备份）来完成。
    *   **策略改进 (Policy Improvement)**: 根据 $V^{\pi_i}$ 计算新的策略 $\pi_{i+1}$。这需要引入**状态-行动价值函数 $Q^{\pi}(s, a)$**：
        $$Q^{\pi}(s, a) = R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a)V^{\pi}(s')$$
        新的策略 $\pi_{i+1}(s)$ 在每个状态 $s$ 下，选择使得 $Q^{\pi_i}(s, a)$ 最大的行动：
        $$\pi_{i+1}(s) = \arg \max_a Q^{\pi_i}(s, a)$$
    *   增加迭代计数 $i = i + 1$。

*   **单调改进**: 策略迭代保证每次迭代都会使策略的价值**单调不减**，$V^{\pi_{i+1}}(s) \geq V^{\pi_i}(s)$。如果当前策略不是最优的，则会严格改进。
*   **收敛性**: 由于状态和行动空间是有限的，确定性策略的数量也是有限的 ($|A|^{|S|}$)，并且策略价值单调不减，因此策略迭代保证在有限步内收敛到最优策略。

### 2.5 价值迭代（VI）(Value Iteration (VI))

价值迭代是另一种直接计算最优价值函数的方法：

1.  **初始化**: 设置 $k=1$。初始化 $V_0(s) = 0$ 对于所有状态 $s$。
2.  **循环**: 直到收敛（例如，$||V_{k+1} - V_k||_{\infty} \leq \epsilon$）。在每次迭代 $k$ 中：
    *   对于每个状态 $s$:
        $$V_{k+1}(s) = \max_a \left[ R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a)V_k(s') \right]$$
    *   增加迭代计数 $k = k + 1$。
*   **贝尔曼备份操作符**: 价值迭代的更新步骤可以看作是应用最优贝尔曼操作符 $B^*$，即 $V_{k+1} = B^*V_k$。
*   **提取最优策略**: 一旦 $V^*(s)$ 收敛，最优策略 $\pi^*(s)$ 可以通过贪婪地选择最大化 $Q^*(s, a)$ 的行动来提取：$$\pi^*(s) = \arg \max_a [ R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a)V^*(s') ]$$

#### 2.5.1 收缩操作符 (Contraction Operator)

*   **定义**: 一个操作符 $O$ 是收缩的，如果它将任意两个价值函数之间的距离缩小。即 $||OV - OV'|| \leq \gamma ||V - V'||$ 对于某个 $\gamma < 1$。
*   **收敛性**: 贝尔曼备份操作符是收缩的，只要折扣因子 $\gamma < 1$。这意味着无论初始价值函数如何，价值迭代都会收敛到唯一的固定点（最优价值函数）。

#### 2.5.2 有限视野价值迭代 (Value Iteration for Finite Horizon H)

*   在有限视野问题中，我们关心的是在剩余 $k$ 个时间步内的最优价值 $V_k(s)$ 和最优策略 $\pi_k(s)$。
*   初始化 $V_0(s) = 0$。循环直到 $k == H$：
    *   $$V_{k+1}(s) = \max_a \left[ R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a)V_k(s') \right]$$
    *   同时，最优策略为：
        $$\pi_{k+1}(s) = \arg \max_a \left[ R(s, a) + \gamma \sum_{s' \in S} P(s'|s, a)V_k(s') \right]$$
*   **注意**: 在有限视野任务中，最优策略通常是**不平稳**的，即它会随着剩余时间步数 $k$ 的变化而变化。

### 2.6 价值迭代 vs 策略迭代 (Value Iteration vs Policy Iteration)

*   **价值迭代**:
    *   计算视野为 $k$ 时的最优价值，并逐步增加 $k$ 直到收敛（无限视野）或达到指定视野 $H$（有限视野）。
    *   可以直接用于计算最优策略。
*   **策略迭代**:
    *   计算给定策略的无限视野价值。
    *   用于选择更好的策略。
    *   与策略梯度（Policy Gradient）方法密切相关，策略梯度是RL中非常流行的一种方法。


## Lecture 3

## 1. 关键要点 (Key Points)

-   **无模型评估的需求**: 在许多现实世界场景中，我们无法直接获取或构建环境的精确动态模型 $P(s'|s,a)$ 和奖励模型 $R(s,a)$。
-   **目标**: 估计给定策略 $\pi$ 下的预期回报 $V^{\pi}(s)$ 或 $Q^{\pi}(s,a)$。
-   **蒙特卡洛（MC）策略评估**:
    *   通过对完整的**回合 (episode)** 进行采样来估算价值。
    *   不依赖于环境模型，也不需要马尔可夫假设（但通常在马尔可夫环境下应用）。
    *   **无偏估计 (Unbiased Estimator)**（首次访问MC），但通常**方差较高 (High Variance)**。
    *   需要每个回合都终止 (episodic settings)。
-   **时序差分（TD）学习**:
    *   结合了蒙特卡洛的**直接经验学习**和动态规划的**自举 (Bootstrapping)** 思想。
    *   **无模型**，可以用于**非回合性 (non-episodic) 的无限视野**设置。
    *   在每个状态-行动-奖励-下一状态 ($s, a, r, s'$) 元组发生后立即更新价值估计。
    *   **有偏估计 (Biased Estimator)**（因为使用估计值来更新估计值），但通常**方差较低 (Lower Variance)**。
    *   TD(0) 更新公式为 $V^{\pi}(S_t) \leftarrow V^{\pi}(S_t) + \alpha([r_t + \gamma V^{\pi}(S_{t+1})] - V^{\pi}(S_t)))$。
-   **确定性等价（Certainty Equivalence）**:
    *   首先通过**最大似然估计 (Maximum Likelihood Estimation, MLE)** 从经验数据中学习一个**模型** $\hat{P}(s'|s,a)$ 和 $\hat{R}(s,a)$。
    *   然后，在这个估计模型上应用传统的**动态规划**算法进行策略评估。
    *   数据效率高，但每次更新需要执行DP，计算成本较高。
-   **评估算法质量的指标**:
    *   **一致性 (Consistency)**: 数据足够多时，估计值是否收敛到真实值。
    *   **计算复杂度 (Computational Complexity)**: 更新估计值的成本。
    *   **内存需求 (Memory Requirements)**。
    *   **统计效率 (Statistical Efficiency)**: 数据量如何影响估计精度。
    *   **经验准确性 (Empirical Accuracy)**: 通常通过均方误差（Mean Squared Error, MSE）评估。

---

## 2. 详细解读 (Detailed Breakdown)

### 2.1 引入无模型策略评估 (Introduction to Model-Free Policy Evaluation)

*   **回顾**: 上次讲座讨论了在已知世界模型（动态模型和奖励模型）的情况下，如何进行策略评估和控制。
*   **本次主题**: 如何在**不知道**环境动态模型和奖励模型的情况下，估计给定策略的预期回报。
*   **从直接经验中评估**: 目标是估计策略 $\\pi$ 的预期回报，仅仅使用从环境中获得的经验数据。
    *   这在许多实际应用中非常重要，因为真实世界的模型往往未知或难以精确建模。
    *   衡量策略评估算法质量的属性包括：一致性、计算复杂度、内存需求、统计效率和经验准确性（通常用均方误差评估）。

### 2.2 蒙特卡洛（MC）策略评估 (Monte Carlo (MC) Policy Evaluation)

*   **基本思想**: 价值等于从遵循策略 $\pi$ 生成的轨迹 $\tau$ 的平均回报。
    $$V^{\pi}(s) = E_{\tau \sim \pi}[G_t | S_t = s]$$
    其中 $G_t$ 是从时间步 $t$ 开始的折扣回报总和：
    $$G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + \dots + \gamma^{T_i-t-1} r_{T_i}$$
*   **工作原理**:
    *   生成大量遵循策略 $\pi$ 的完整回合（从初始状态到终止状态）。
    *   对于每个回合中访问过的状态 $s$，计算其在该回合中的回报 $G_t$。
    *   **首次访问MC (First-Visit MC)**: 仅在回合中首次访问状态 $s$ 时，才使用其后的回报来更新 $V(s)$。
    *   **每次访问MC (Every-Visit MC)**: 在回合中每次访问状态 $s$ 时，都使用其后的回报来更新 $V(s)$。
    *   为什么会多次访问？
    *   **增量式MC (Incremental MC)**: 允许在线更新价值估计，无需等待所有回合完成。
        $$V^{\pi}(s) \leftarrow V^{\pi}(s) + \alpha(G_{i,t} - V^{\pi}(s))$$
        其中 $G_{i,t}$ 是第 $i$ 个回合中从状态 $s_t$ 开始的回报，$\alpha$ 是学习率。对于收敛，学习率 $\alpha_n(s_j)$ 需要满足条件：$\sum_{n=1}^{\infty} \alpha_n(s_j) = \infty$ 且 $\sum_{n=1}^{\infty} \alpha_n^2(s_j) < \infty$。
*   **MC方法的属性**:
    *   **优点**:
        *   不需要MDP的动态模型或奖励模型。
        *   不假设状态是马尔可夫的（可以直接处理原始观察）。
        *   易于理解和实现。
        *   首次访问MC是**真实价值 $E_{\pi}[G_t|S_t=s]$ 的无偏估计**。根据大数定律，当访问次数 $N(s) \to \infty$ 时，$V^{\pi}(s) \to E_{\pi}[G_t|S_t=s]$。
    *   **局限性**:
        *   通常是**高方差估计**，因为回报 $G_t$ 本身就是随机变量的和，可能包含很多噪声。
        *   需要**回合结束 (episodic setting)**：必须等到回合结束才能计算完整的回报来更新价值。这在连续任务中不适用。
        *   数据获取成本高或风险高时，MC可能不实用。

### 2.3 时序差分（TD）学习 (Temporal Difference (TD) Learning)

*   **核心思想**: "如果说有一种思想是强化学习的核心和创新点，那无疑是时序差分（TD）学习。"——Sutton and Barto 2017。
*   **MC + DP**: TD学习结合了蒙特卡洛的**从经验中学习**和动态规划的**自举 (bootstrapping)** 思想。
*   **自举 (Bootstrapping)**: TD更新使用其自身的当前价值估计来估计下一个状态的价值，而不是等待实际的回报。动态规划也使用自举。
    *   $$V^{\pi}(S_t) \leftarrow V^{\pi}(S_t) + \alpha \left( [r_t + \gamma V^{\pi}(S_{t+1})] - V^{\pi}(S_t) \right)$$
    *   其中，$r_t + \gamma V^{\pi}(S_{t+1})$ 称为**TD目标 (TD target)**，$[r_t + \gamma V^{\pi}(S_{t+1})] - V^{\pi}(S_t)$ 称为**TD误差 (TD error)** $\delta_t$。
*   **TD(0) 学习算法**:
    1.  **输入**: 学习率 $\alpha$。
    2.  **初始化**: $V^{\pi}(s) = 0$ 对所有 $s \in S$。
    3.  **循环**:
        *   采样一个元组 $(S_t, A_t, r_t, S_{t+1})$。
        *   更新 $V^{\pi}(S_t) \leftarrow V^{\pi}(S_t) + \alpha([r_t + \gamma V^{\pi}(S_{t+1})] - V^{\pi}(S_t)))$。
*   **TD方法的属性**:
    *   **优点**:
        *   **无模型**: 不需要动态模型和奖励模型。
        *   **立即更新**: 在每个 $(s, a, r, s')$ 元组发生后立即更新价值估计，无需等待回合结束。
        *   可以用于**非回合性 (non-episodic) 的无限视野**设置。
        *   通常比MC具有**更低的方差**，因为其TD目标只涉及一步随机性。
        *   隐式地利用了马尔可夫结构（如果存在）。
    *   **局限性**:
        *   是**有偏估计**，因为TD目标中使用了估计的价值 $V^{\\pi}(S_{t+1})$。
        *   尽管有偏，但在表格表示下，TD(0) 在收敛到真实值方面通常是一致的。

### 2.4 评估策略估计方法的质量 (Evaluation of the Quality of a Policy Estimation Approach)

*   **偏差 (Bias)**: 估计量的期望值与真实参数之间的差异。
    $$Bias_{\theta}(\hat{\theta}) = E_{X|\theta}[\hat{\theta}] - \theta$$
    例如，首次访问MC是无偏的，而TD是带偏差的。
*   **方差 (Variance)**: 估计量在不同样本下的变动程度。
    $$Var(\hat{\theta}) = E_{X|\theta}[(\hat{\theta} - E[\hat{\theta}])^2]$$
    MC通常方差较高，TD方差较低。
*   **均方误差 (Mean Squared Error, MSE)**: 偏差和方差的综合度量。
    $$MSE(\hat{\theta}) = Var(\hat{\theta}) + Bias_{\theta}(\hat{\theta})^2$$
    通常在实际中，我们寻求低MSE的估计器。
*   **一致性 (Consistency)**: 随着数据量 $n$ 的增加，估计值 $\hat{\theta}_n$ 是否收敛到真实参数 $\theta$。
    $$\lim_{n \to \infty} Pr(|\hat{\theta}_n - \theta| > \epsilon) = 0 \quad \text{for any } \epsilon > 0$$
    MC和TD在特定条件下都是一致的。

### 2.5 批处理蒙特卡洛和时序差分 (Batch MC and TD)

*   **离线解决方案 (Offline Solution)**: 在给定有限数据集（固定数量的回合 $K$）的情况下进行价值估计。
*   **批处理MC**: 在批处理设置中，蒙特卡洛收敛到最小化观测回报的均方误差。例如，在AB示例中，$V(A)=0$（MC）。
*   **批处理TD(0)**: 在批处理设置中，TD(0) 收敛到**最大似然模型估计的MDP所对应的DP策略价值函数**。这与"确定性等价"是相同的。例如，在AB示例中，$V(A)=0.75$（TD）。
*   **这意味着**:
    *   MC的目标是与经验数据中观察到的实际回报的平均值尽可能接近。
    *   TD的目标是与一个通过数据推断出的"最佳"马尔可夫模型下的贝尔曼方程解尽可能接近。

### 2.6 模型无关策略评估算法的重要特性 (Important Properties to Evaluate Model-free Policy Evaluation Algorithms)

*   **数据效率与计算效率**:
    *   简单TD(0) 每次更新是 $O(1)$ 操作；一个长度为 $L$ 的回合是 $O(L)$。
    *   MC必须等到回合结束才能更新，也是 $O(L)$。
    *   MC在某些情况下数据效率更高（因为它使用整个轨迹），但TD可以更有效地利用马尔可夫结构。
*   **是否利用马尔可夫结构**:
    *   TD会利用马尔可夫结构（通过 $S_{t+1}$），这在马尔可夫领域很有帮助。
    *   确定性等价与动态规划也利用马尔可夫结构。

---

## 3. 结论与启示 (Conclusion & Implications)

-   **核心**: 当环境模型未知时，蒙特卡洛和时序差分学习是两种强大的无模型策略评估方法。
-   **选择**:
    *   **MC**：适合回合性任务，当需要**无偏估计**且能获得完整回合的回报时。方差高是其主要缺点。
    *   **TD**：可用于**回合性和非回合性任务**，在每个时间步进行**在线学习**，通常具有**更低的方差**。虽然有偏，但在实践中往往表现更好。
    *   **确定性等价**: 通过显式估计模型再进行DP，提供数据高效且一致的估计，但计算成本高。
-   **权衡**: 选择哪种方法取决于具体任务的特性（回合性/非回合性）、对模型知识的假设、对偏差和方差的容忍度以及计算资源。理解这些方法的内在工作原理和统计特性是至关重要的。


## Lecture 4

## 1. 关键要点 (Key Points)

-   **无模型控制**: 在不了解环境动态模型和奖励模型的情况下，学习如何采取最优行动。
-   **探索与利用的权衡 (Exploration vs. Exploitation)**: 为了找到最优策略，智能体需要探索新的行动（探索），但也要利用已知的高回报行动（利用）。
-   **$\epsilon$-贪婪策略 ($\epsilon$-greedy Policies)**: 一种平衡探索和利用的简单方法，以 $1-\epsilon$ 的概率选择当前最优行动，以 $\epsilon$ 的概率随机选择行动。它保证了策略的单调改进。
-   **GLIE条件 (Greedy in the Limit of Infinite Exploration)**: 确保收敛到最优策略的条件，要求所有状态-行动对都被无限次访问，并且探索率 $\epsilon$ 随时间衰减到0（例如 $\epsilon_i = 1/i$）。
-   **蒙特卡洛（MC）控制**: 将MC策略评估与$\epsilon$-贪婪策略改进相结合，通过采样完整的回合来学习Q值和改进策略。在GLIE条件下，表格型MC控制收敛到最优Q值。
-   **时序差分（TD）控制**:
    *   **SARSA（同策略学习）**: 智能体使用其当前正在遵循的策略（行为策略）来生成数据并更新其Q值。更新公式为 $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha([r_{t+1} + \gamma Q(S_{t+1}, A_{t+1})] - Q(S_t, A_t)))$。
    *   **Q-learning（异策略学习）**: 智能体使用一个行为策略来生成数据，但学习的是一个不同的（通常是贪婪的）目标策略的Q值。更新公式为 $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha([r_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a')] - Q(S_t, A_t)))$。
-   **价值函数逼近 (Value Function Approximation, VFA)**:
    *   动机: 解决状态-行动空间过大导致无法使用表格法存储所有价值的问题，通过参数化函数来估计价值函数，实现泛化。
    *   方法: 通常使用随机梯度下降（SGD）来最小化估计值与目标值之间的均方误差。
-   **"致命三元组" (Deadly Triad)**: 由**函数逼近**、**自举 (Bootstrapping)** 和**异策略学习 (Off-policy Learning)** 组合导致的不稳定性，可能使RL算法发散。Q-learning + VFA 容易受到此问题影响。
-   **深度Q网络（Deep Q-Network, DQN）**:
    *   核心创新: 结合深度神经网络进行VFA，并通过**经验回放**和**固定Q目标**解决稳定性问题。
    *   **经验回放**: 将历史转换数据存储在回放缓冲区中，并从中随机采样 mini-batch 进行学习，以打破样本间的相关性。
    *   **固定Q目标**: 使用一个独立的、定期更新的"目标网络"来计算TD目标，从而使目标更加稳定。
    *   在Atari等复杂任务中取得超越人类的表现。

---

## 2. 详细解读 (Detailed Breakdown)

### 2.1 无模型策略迭代 (Model-free Policy Iteration)

*   **回顾策略迭代**: 策略迭代通过交替进行策略评估和策略改进来找到最优策略。在无模型设置中，我们无法直接计算策略的Q值或V值，因为我们不知道环境模型。
*   **挑战**: 如果当前策略是确定性的，我们可能无法访问某些状态-行动对的数据，从而无法评估其Q值。
*   **解决方案**: 需要巧妙地交错策略评估和改进步骤，并使用估计的Q值。同时，必须解决**探索 (Exploration)** 问题。
*   **探索与利用 (Exploration vs. Exploitation)**: 学习如何最大化未来总预期回报需要我们尝试新的行动（探索），但也需要利用过去经验中已知的高回报行动（利用）。

### 2.2 $\epsilon$-贪婪策略 ($\epsilon$-Greedy Policies)

*   **定义**: 一种简单的平衡探索与利用的策略。给定一个状态-行动价值函数 $Q(s, a)$ 和一个探索参数 $\epsilon \in [0, 1]$：
    *   以 $1 - \epsilon$ 的概率选择使 $Q(s, a)$ 最大的行动（贪婪行动）。
    *   以 $\epsilon$ 的概率从所有行动中**均匀随机**选择一个。
    *   这保证了所有行动都有被选中的机会，从而实现探索。
*   **策略改进**: $\epsilon$-贪婪策略也支持**单调策略改进**。这意味着，对于任何$\epsilon$-贪婪策略 $\pi_i$，其相对于 $Q^{\pi_i}$ 的 $\epsilon$-贪婪策略 $\pi_{i+1}$ 将保证 $V^{\pi_{i+1}}(s) \ge V^{\pi_i}(s)$，并且在非最优情况下严格改进。

### 2.3 蒙特卡洛（MC）控制 (Monte Carlo Control)

*   **目标**: 在无模型设置下，通过蒙特卡洛方法来寻找最优策略，通常是学习最优状态-行动价值函数 $Q^*(s, a)$。
*   **算法（GLIE MC Control）**:
    1.  **初始化**: $Q(s, a) = 0$, $N(s, a) = 0$ 对所有 $(s, a)$，设置初始探索率 $\epsilon=1$ 和回合计数 $k=1$。
    2.  **循环**: 当策略不再改变（$||\pi_i - \pi_{i-1}||_1 = 0$）时停止。在每次迭代 $i$ 中：
        *   **策略生成**: 使用当前策略 $\pi_k$ （例如，$\epsilon$-贪婪策略）生成一个完整回合：$(S_{k,1}, A_{k,1}, R_{k,1}, \dots, S_{k,T_k})$。
        *   **Q值更新**: 对于回合中首次访问的每个状态-行动对 $(s_t, a_t)$，计算其回报 $G_{k,t}$。然后更新计数和Q值：
            *   $N(s_t, a_t) \leftarrow N(s_t, a_t) + 1$
            *   $Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \frac{1}{N(s_t, a_t)}(G_{k,t} - Q(s_t, a_t))$
        *   **策略改进**: 更新策略 $\pi_k$ 为新的 $Q$ 值对应的 $\epsilon$-贪婪策略。
        *   **探索率衰减**: 降低 $\epsilon$ 值，例如 $\epsilon_k = 1/k$，以满足**GLIE条件**。
*   **GLIE条件 (Greedy in the Limit of Infinite Exploration)**:
    1.  所有状态-行动对都被无限次访问：$\lim_{k \to \infty} N_k(s, a) \to \infty$。
    2.  行为策略最终收敛到贪婪策略：$\lim_{k \to \infty} \epsilon_k = 0$。
*   **收敛性定理**: 在GLIE条件下，表格型蒙特卡洛控制收敛到最优状态-行动价值函数 $Q^*(s, a)$。

### 2.4 时序差分（TD）控制 (Temporal Difference (TD) Methods for Control)

*   **无模型策略迭代与TD**: 与MC方法类似，TD方法也可以用于无模型策略迭代。核心思想是使用TD方法来估计Q值，而不是MC方法。
*   **SARSA（同策略TD控制）**:
    *   **同策略 (On-policy)**: 智能体通过**当前正在遵循的策略**来生成经验（$S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$）并更新Q值。
    *   更新规则:
        $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha([r_{t+1} + \gamma Q(S_{t+1}, A_{t+1})] - Q(S_t, A_t)))$$
        注意 $A_{t+1}$ 是根据**当前策略 $\pi$** 从 $S_{t+1}$ 采样得到的。
    *   **收敛性**: 对于有限状态和行动的MDP，SARSA在GLIE条件（策略序列 $\pi_k(a|s)$ 满足GLIE，且步长 $\alpha_t$ 满足Robbins-Munro条件，如 $\sum_{t=1}^{\infty} \alpha_t = \infty$ 且 $\sum_{t=1}^{\infty} \alpha_t^2 < \infty$，例如 $\alpha_t = 1/t$）下，收敛到最优状态-行动价值函数 $Q^*(s,a)$。
    *   SARSA的收敛性依赖于随机逼近、正确衰减的步长、贝尔曼备份的收缩性质以及有界的回报和价值函数。

*   **Q-learning（异策略TD控制）**:
    *   **异策略 (Off-policy)**: 智能体使用一个**行为策略 (behavior policy)** $\pi_b$（例如 $\epsilon$-贪婪策略）来生成经验数据，但学习的是一个**不同的目标策略 (target policy)** $\pi^*$（通常是基于当前Q值进行贪婪选择的最优策略）的Q值。
    *   更新规则:
        $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha([r_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a')] - Q(S_t, A_t)))$$
        注意TD目标中的 $\max_{a'}$ 表示它自举的是在下一状态 $S_{t+1}$ 下能够获得的**最优未来Q值**，无论智能体实际执行了哪个行动。
    *   Q-learning 直接学习最优Q值 $Q^*$。只要所有状态-行动对都能被行为策略访问到，Q-learning就能收敛到 $Q^*$。
    *   **同策略 vs 异策略总结**:
        *   **同策略学习 (On-policy Learning)**: 从遵循**同一策略**所获得的经验中估计和改进该策略。
        *   **异策略学习 (Off-policy Learning)**: 从遵循**一个不同策略**所获得的经验中估计和改进策略。

### 2.5 价值函数逼近（VFA）(Value Function Approximation (VFA))

*   **动机**: 当状态空间或行动空间太大时（例如，连续空间、高维离散空间），无法使用表格来存储每个状态或状态-行动对的价值。VFA通过学习一个**参数化函数**来估计价值函数，从而实现对未访问状态的**泛化**。
*   **目标**: 找到参数向量 $w$ 来最小化近似价值函数 $\hat{Q}(s, a; w)$ 与真实价值函数 $Q^{\pi}(s, a)$ 之间的损失（通常是均方误差）：
    $$J(w) = E_{\pi}[(Q^{\pi}(s, a) - \hat{Q}(s, a; w))^2]$$
*   **随机梯度下降 (Stochastic Gradient Descent, SGD)**: 普遍用于寻找局部最小值。参数 $w$ 的更新规则是沿着损失函数的负梯度方向：
    $$\Delta w = -\frac{1}{2}\alpha \nabla_w J(w)$$
    其中 $\nabla_w J(w) = E_{\pi}[-(Q^{\pi}(s, a) - \hat{Q}(s, a; w))\nabla_w \hat{Q}(s, a; w)]$。在实践中，SGD使用有限数量（通常是一个）的样本来计算近似梯度。
*   **MC VFA策略评估**: 使用MC方法中的回报 $G_t$ 作为目标来训练价值函数逼近器 $\hat{Q}(s, a; w)$。
*   **TD(0) VFA策略评估**: 同样使用SGD更新参数 $w$，但目标是TD目标 $r + \gamma \hat{V}^{\pi}(s'; w)$。
*   **关键区别**: VFA将策略评估的更新步骤转变为函数逼近的拟合过程。

### 2.6 控制与价值函数逼近 (Control using Value Function Approximation)

*   **挑战**: 当结合**函数逼近**、**自举**和**异策略学习**时，RL算法可能变得**不稳定**。这种组合被称为"致命三元组" (Deadly Triad)，可能导致Q值估计的震荡或发散。Q-learning + 神经网络（作为函数逼近器）就属于这种情况。

### 2.7 深度Q网络（Deep Q-Network, DQN）(Deep Q-Learning)

*   **背景**: DQN是Google DeepMind在2015年提出的一种深度强化学习算法，它成功地在Atari 2600游戏中达到了人类水平，解决了"致命三元组"带来的稳定性问题。
*   **核心创新**:
    1.  **经验回放 (Experience Replay)**:
        *   目的: 打破连续样本之间的相关性，使训练数据更接近独立同分布，从而提高学习稳定性。
        *   机制: 智能体将每次交互的转换元组 $(s_t, a_t, r_{t+1}, s_{t+1})$ 存储在一个名为**回放缓冲区 (Replay Buffer)** 的数据集 $\mathcal{D}$ 中。在训练时，随机从 $\mathcal{D}$ 中采样一个 mini-batch 进行Q值更新。
        *   优点: 提高数据效率（可以重复利用经验），降低样本相关性。
    2.  **固定Q目标 (Fixed Q-Targets)**:
        *   目的: 解决TD目标不稳定导致训练发散的问题。
        *   机制: 引入一个独立的**目标网络 (Target Network)** $Q(\cdot; w^-)$ 来计算TD目标。这个目标网络的参数 $w^-$ 会定期（例如每 $C$ 步）从当前学习网络 $Q(\cdot; w)$ 的参数 $w$ 复制过来，但在两次复制之间保持固定。
        *   Q-learning更新公式变为：
            $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha([r_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a'; w^-)] - Q(S_t, A_t)))$$
        *   优点: 使TD目标更加稳定，减少了目标Q值在学习过程中的频繁变动。
*   **DQN架构**:
    *   使用卷积神经网络 (CNN) 作为Q函数逼近器，直接从原始像素输入（Atari游戏帧）学习Q值。
    *   输入状态 $s$ 是最近几帧（例如4帧）原始像素的堆叠。
    *   输出是所有可能行动的Q值。
    *   网络架构和超参数通常在不同游戏之间保持固定。
*   **成功因素**: 经验回放和固定Q目标被证明是DQN成功的关键。仅使用深度神经网络有时甚至会损害性能。
*   **后续改进**: DQN也启发了许多改进，例如：
    *   **Double DQN**: 解决Q值过高估计问题。
    *   **Prioritized Replay**: 更有效地回放"重要"的经验。
    *   **Dueling DQN**: 改进网络架构以更好地分离状态价值和优势函数。
